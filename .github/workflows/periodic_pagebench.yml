name: Periodic pagebench performance test on dedicated EC2 machine in eu-central-1 region

on:
  schedule:
    # * is a special character in YAML so you have to quote this string
    #        ┌───────────── minute (0 - 59)
    #        │   ┌───────────── hour (0 - 23)
    #        │   │ ┌───────────── day of the month (1 - 31)
    #        │   │ │ ┌───────────── month (1 - 12 or JAN-DEC)
    #        │   │ │ │ ┌───────────── day of the week (0 - 6 or SUN-SAT)
    - cron: '0 */3 * * *' # Runs every 3 hours
  workflow_dispatch: # Allows manual triggering of the workflow
    inputs:
      commit_hash:
        type: string
        description: 'The long neon repo commit hash for the system under test (pageserver) to be tested.'
        required: false
        default: ''
      recreate_snapshots:
        type: boolean
        description: 'Recreate snapshots - !!!WARNING!!! We should only recreate snapshots if the previous ones are no longer compatible. Otherwise benchmarking results are not comparable across runs.'
        required: false
        default: false

defaults:
  run:
    shell: bash -euo pipefail {0}

concurrency:
  group: ${{ github.workflow }}
  cancel-in-progress: false

permissions:
  contents: read

jobs:
  run_periodic_pagebench_test:
    permissions:
      id-token: write # aws-actions/configure-aws-credentials
      statuses: write
      contents: write
      pull-requests: write
    runs-on: [ self-hosted, unit-perf ]
    container:
      image: ghcr.io/neondatabase/build-tools:pinned-bookworm
      credentials:
        username: ${{ github.actor }}
        password: ${{ secrets.GITHUB_TOKEN }}
      options: --init
    timeout-minutes: 360  # Set the timeout to 6 hours
    env:
      RUN_ID: ${{ github.run_id }}
      DEFAULT_PG_VERSION: 16
      BUILD_TYPE: release
      RUST_BACKTRACE: 1
      NEON_ENV_BUILDER_USE_OVERLAYFS_FOR_SNAPSHOTS: 1
      S3_BUCKET: neon-github-public-dev
    steps:
    # we don't need the neon source code because we run everything remotely
    # however we still need the local github actions to run the allure step below
    - name: Harden the runner (Audit all outbound calls)
      uses: step-security/harden-runner@4d991eb9b905ef189e4c376166672c3f2f230481 # v2.11.0
      with:
        egress-policy: audit

    - name: Set up the environment which depends on $RUNNER_TEMP on nvme drive
      id: set-env
      shell: bash -euxo pipefail {0}
      run: |
        # Set up the environment variables
        echo "NEON_DIR=${RUNNER_TEMP}/neon" >> $GITHUB_ENV
        echo "BACKUP_DIR=${RUNNER_TEMP}/instance_store/saved_snapshots" >> $GITHUB_ENV
        echo "TEST_OUTPUT=${RUNNER_TEMP}/neon/test_output" >> $GITHUB_ENV
        echo "ALLURE_DIR=${RUNNER_TEMP}/neon/test_output/allure-results" >> $GITHUB_ENV
        echo "ALLURE_RESULTS_DIR=${RUNNER_TEMP}/neon/test_output/allure-results/results" >> $GITHUB_ENV

    - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2

    - uses: aws-actions/configure-aws-credentials@e3dd6a429d7300a6a4c196c26e071d42e0343502 # v4.0.2
      with:
        aws-region: eu-central-1
        role-to-assume: ${{ vars.DEV_AWS_OIDC_ROLE_ARN }}
        role-duration-seconds: 18000 # max 5 hours (needed in case commit hash is still being built)
    - name: Determine commit hash
      env:
        INPUT_COMMIT_HASH: ${{ github.event.inputs.commit_hash }}
      run: |
        if [ -z "$INPUT_COMMIT_HASH" ]; then
          echo "COMMIT_HASH=$(curl -s https://api.github.com/repos/neondatabase/neon/commits/main | jq -r '.sha')" >> $GITHUB_ENV
          echo "COMMIT_HASH_TYPE=latest" >> $GITHUB_ENV
        else
          echo "COMMIT_HASH=$INPUT_COMMIT_HASH" >> $GITHUB_ENV
          echo "COMMIT_HASH_TYPE=manual" >> $GITHUB_ENV
        fi

    # does not reuse ./.github/actions/download because we need to download the artifact for the given commit hash
    # example artifact
    # s3://neon-github-public-dev/artifacts/48b870bc078bd2c450eb7b468e743b9c118549bf/15036827400/1/neon-Linux-X64-release-artifact.tar.zst /instance_store/artifacts/neon-Linux-release-artifact.tar.zst
    - name: Determine artifact S3_KEY for commit hash ${COMMIT_HASH} and download and extract artifact
      id: artifact_prefix
      shell: bash -euxo pipefail {0}
      env:
        ARCHIVE: ${{ runner.temp }}/downloads/neon-${{ runner.os }}-${{ runner.arch }}-release-artifact.tar.zst
        COMMIT_HASH: ${{ env.COMMIT_HASH }}
        COMMIT_HASH_TYPE: ${{ env.COMMIT_HASH_TYPE }}
      run: |
        attempt=0
        max_attempts=24 # 5 minutes * 24 = 2 hours

        while [ $attempt -lt $max_attempts ]; do
          set +e # the following command will fail until the artifacts are available
          S3_KEY=$(aws s3api list-objects-v2 --bucket $S3_BUCKET --prefix artifacts/$COMMIT_HASH/ | jq -r '.Contents[]?.Key' | grep neon-${{ runner.os }}-${{ runner.arch }}-release-artifact.tar.zst | sort --version-sort | tail -1)
          set -e

          if [ ! -z "$S3_KEY" ]; then
            echo "Artifact found: $S3_KEY"
            echo "S3_KEY=$S3_KEY" >> $GITHUB_ENV
            break
          fi
          
          # Increment attempt counter and sleep for 5 minutes
          attempt=$((attempt + 1))
          echo "Attempt $attempt of $max_attempts to find artifacts in S3 bucket s3://$S3_BUCKET/artifacts/$COMMIT_HASH failed. Retrying in 5 minutes..."
          sleep 300 # Sleep for 5 minutes
        done

        if [ -z "$S3_KEY" ]; then
          echo "Error: artifact not found in S3 bucket s3://$S3_BUCKET/artifacts/$COMMIT_HASH" after 2 hours
        else
          mkdir -p $(dirname $ARCHIVE)
          time aws s3 cp --only-show-errors s3://$S3_BUCKET/${S3_KEY} ${ARCHIVE}
          mkdir -p ${NEON_DIR}
          time tar -xf ${ARCHIVE} -C ${NEON_DIR}
          rm -f ${ARCHIVE}
        fi

    - name: Download snapshots from S3
      if: ${{ github.event_name != 'workflow_dispatch' || github.event.inputs.recreate_snapshots == 'false' || github.event.inputs.recreate_snapshots == '' }}
      id: download_snapshots
      shell: bash -euxo pipefail {0}
      run: |
        # Download the snapshots from S3
        mkdir -p ${TEST_OUTPUT}
        mkdir -p $BACKUP_DIR
        cd $BACKUP_DIR
        mkdir parts
        cd parts
        PART=$(aws s3api list-objects-v2 --bucket $S3_BUCKET --prefix performance/pagebench/ \
          | jq -r '.Contents[]?.Key' \
          | grep -E 'shared-snapshots-[0-9]{4}-[0-9]{2}-[0-9]{2}' \
          | sort \
          | tail -1)
        echo "Latest PART: $PART"
        if [[ -z "$PART" ]]; then
          echo "ERROR: No matching S3 key found" >&2
          exit 1
        fi
        S3_KEY=$(dirname $PART)
        time aws s3 cp --only-show-errors --recursive s3://${S3_BUCKET}/$S3_KEY/ .
        cd $TEST_OUTPUT
        time cat $BACKUP_DIR/parts/* | zstdcat | tar --extract --preserve-permissions

    - name: Cleanup Test Resources
      if: always()
      shell: bash -euxo pipefail {0}
      env:
        ARCHIVE: ${{ runner.temp }}/downloads/neon-${{ runner.os }}-${{ runner.arch }}-release-artifact.tar.zst
      run: |
        # Cleanup the test resources
        if [ -d "${BACKUP_DIR}" ]; then
          rm -rf ${BACKUP_DIR}
        fi
        if [ -d "${TEST_OUTPUT}" ]; then
          rm -rf ${TEST_OUTPUT}
        fi
        if [ -d "${NEON_DIR}" ]; then
          rm -rf ${NEON_DIR}
        fi
        rm -rf $(dirname $ARCHIVE)

